{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1- Detect and crop license plates in car images.\n",
        "\n",
        "https://www.kaggle.com/datasets/andrewmvd/car-plate-detection?select=annotations\n",
        "\n",
        "\n",
        "2- detect and crop characters from license plates images.\n",
        "\n",
        "3- classify characters and print text output."
      ],
      "metadata": {
        "id": "oQ5MYdOoM3UP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pdn42kFymQoT",
        "outputId": "cff5de01-7fbe-4834-e1a5-800550e3f5ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "# !pip install tensorflow==2.4.0\n",
        "# !pip install tensorflow-gpu==2.4.0\n",
        "# !pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0\n",
        "# !pip install imageai --upgrade\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image\n",
        "from PIL import ImageOps\n",
        "import PIL\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import tensorboard\n",
        "import IPython\n",
        "import sklearn\n",
        "import cv2\n",
        "import subprocess\n",
        "import sys\n",
        "from imageai.Classification import ImageClassification\n",
        "from imageai.Detection.Custom import DetectionModelTrainer\n",
        "from imageai.Detection.Custom import CustomObjectDetection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trainer\n",
        "\n",
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/content/drive/MyDrive/imageai/LPDS\")\n",
        "trainer.setTrainConfig(object_names_array=[\"licence\"], batch_size=8, num_experiments=30, train_from_pretrained_model=\"/content/drive/MyDrive/imageai/LPDS/models/detection_model-ex-018--loss-0014.416.h5\")\n",
        "trainer.trainModel()"
      ],
      "metadata": {
        "id": "XJKc9CNPWaTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64c4e8c-2ec7-4b26-90e0-205aef1e17e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating anchor boxes for training images and annotation...\n",
            "Average IOU for 9 anchors: 0.74\n",
            "Anchor Boxes generated.\n",
            "Detection configuration saved in  /content/drive/MyDrive/imageai/LPDS/json/detection_config.json\n",
            "Evaluating over 101 samples taken from /content/drive/MyDrive/imageai/LPDS/validation\n",
            "Training over 332 samples  given at /content/drive/MyDrive/imageai/LPDS/train\n",
            "Training on: \t['licence']\n",
            "Training with Batch Size:  8\n",
            "Number of Training Samples:  332\n",
            "Number of Validation Samples:  101\n",
            "Number of Experiments:  30\n",
            "Training with transfer learning from pretrained Model\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer YoloLayer has arguments in `__init__` and therefore must override `get_config`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "336/336 [==============================] - 291s 851ms/step - loss: 38.3340 - yolo_layer_loss: 4.4458 - yolo_layer_1_loss: 11.1007 - yolo_layer_2_loss: 11.3102 - val_loss: 34.3537 - val_yolo_layer_loss: 3.5735 - val_yolo_layer_1_loss: 9.3404 - val_yolo_layer_2_loss: 10.9266\n",
            "Epoch 2/30\n",
            "336/336 [==============================] - 280s 830ms/step - loss: 31.7317 - yolo_layer_loss: 3.4332 - yolo_layer_1_loss: 8.5602 - yolo_layer_2_loss: 10.1926 - val_loss: 27.1701 - val_yolo_layer_loss: 3.0571 - val_yolo_layer_1_loss: 7.1156 - val_yolo_layer_2_loss: 9.5034\n",
            "Epoch 3/30\n",
            "336/336 [==============================] - 282s 836ms/step - loss: 26.3966 - yolo_layer_loss: 3.6387 - yolo_layer_1_loss: 6.6840 - yolo_layer_2_loss: 8.9196 - val_loss: 23.8929 - val_yolo_layer_loss: 3.7507 - val_yolo_layer_1_loss: 5.6227 - val_yolo_layer_2_loss: 7.9731\n",
            "Epoch 4/30\n",
            "336/336 [==============================] - 281s 833ms/step - loss: 24.2305 - yolo_layer_loss: 4.0056 - yolo_layer_1_loss: 5.4600 - yolo_layer_2_loss: 8.3205 - val_loss: 21.9220 - val_yolo_layer_loss: 3.2956 - val_yolo_layer_1_loss: 5.5269 - val_yolo_layer_2_loss: 6.9087\n",
            "Epoch 5/30\n",
            "336/336 [==============================] - 272s 806ms/step - loss: 22.2905 - yolo_layer_loss: 3.3748 - yolo_layer_1_loss: 5.0732 - yolo_layer_2_loss: 7.7108 - val_loss: 20.5860 - val_yolo_layer_loss: 3.3160 - val_yolo_layer_1_loss: 4.9043 - val_yolo_layer_2_loss: 6.3579\n",
            "Epoch 6/30\n",
            "336/336 [==============================] - 279s 828ms/step - loss: 21.5692 - yolo_layer_loss: 3.6552 - yolo_layer_1_loss: 4.8240 - yolo_layer_2_loss: 7.1313 - val_loss: 21.2536 - val_yolo_layer_loss: 4.5748 - val_yolo_layer_1_loss: 4.7526 - val_yolo_layer_2_loss: 6.0999\n",
            "Epoch 7/30\n",
            "336/336 [==============================] - 281s 833ms/step - loss: 21.3076 - yolo_layer_loss: 4.1169 - yolo_layer_1_loss: 4.6368 - yolo_layer_2_loss: 6.7650 - val_loss: 18.5592 - val_yolo_layer_loss: 2.8867 - val_yolo_layer_1_loss: 4.1116 - val_yolo_layer_2_loss: 5.8600\n",
            "Epoch 8/30\n",
            "336/336 [==============================] - 281s 831ms/step - loss: 19.7103 - yolo_layer_loss: 3.4183 - yolo_layer_1_loss: 4.2799 - yolo_layer_2_loss: 6.3423 - val_loss: 19.0689 - val_yolo_layer_loss: 3.1462 - val_yolo_layer_1_loss: 4.5124 - val_yolo_layer_2_loss: 5.8326\n",
            "Epoch 9/30\n",
            "336/336 [==============================] - 283s 840ms/step - loss: 19.9407 - yolo_layer_loss: 3.8361 - yolo_layer_1_loss: 4.3050 - yolo_layer_2_loss: 6.2500 - val_loss: 22.9817 - val_yolo_layer_loss: 6.3327 - val_yolo_layer_1_loss: 5.8033 - val_yolo_layer_2_loss: 5.3747\n",
            "Epoch 10/30\n",
            "336/336 [==============================] - 277s 820ms/step - loss: 19.9710 - yolo_layer_loss: 4.0984 - yolo_layer_1_loss: 4.3451 - yolo_layer_2_loss: 6.0817 - val_loss: 19.3507 - val_yolo_layer_loss: 3.3203 - val_yolo_layer_1_loss: 4.7686 - val_yolo_layer_2_loss: 5.8912\n",
            "Epoch 11/30\n",
            "336/336 [==============================] - 269s 797ms/step - loss: 18.9652 - yolo_layer_loss: 3.7701 - yolo_layer_1_loss: 3.8349 - yolo_layer_2_loss: 6.0121 - val_loss: 20.7087 - val_yolo_layer_loss: 6.3952 - val_yolo_layer_1_loss: 3.7151 - val_yolo_layer_2_loss: 5.3213\n",
            "Epoch 12/30\n",
            "336/336 [==============================] - 275s 817ms/step - loss: 18.0038 - yolo_layer_loss: 3.5091 - yolo_layer_1_loss: 3.5228 - yolo_layer_2_loss: 5.7194 - val_loss: 16.3380 - val_yolo_layer_loss: 2.5220 - val_yolo_layer_1_loss: 3.2514 - val_yolo_layer_2_loss: 5.3842\n",
            "Epoch 13/30\n",
            "336/336 [==============================] - 268s 794ms/step - loss: 18.2238 - yolo_layer_loss: 3.5458 - yolo_layer_1_loss: 3.6540 - yolo_layer_2_loss: 5.8695 - val_loss: 17.8261 - val_yolo_layer_loss: 4.4061 - val_yolo_layer_1_loss: 3.1709 - val_yolo_layer_2_loss: 5.1651\n",
            "Epoch 14/30\n",
            "336/336 [==============================] - 272s 807ms/step - loss: 18.3322 - yolo_layer_loss: 4.1727 - yolo_layer_1_loss: 3.6618 - yolo_layer_2_loss: 5.4314 - val_loss: 28.7214 - val_yolo_layer_loss: 12.0303 - val_yolo_layer_1_loss: 6.2268 - val_yolo_layer_2_loss: 5.4615\n",
            "Epoch 15/30\n",
            "336/336 [==============================] - 282s 838ms/step - loss: 17.3029 - yolo_layer_loss: 3.3084 - yolo_layer_1_loss: 3.4851 - yolo_layer_2_loss: 5.5256 - val_loss: 17.4237 - val_yolo_layer_loss: 3.4535 - val_yolo_layer_1_loss: 3.9633 - val_yolo_layer_2_loss: 5.0910\n",
            "Epoch 16/30\n",
            "336/336 [==============================] - 281s 832ms/step - loss: 17.5500 - yolo_layer_loss: 3.6818 - yolo_layer_1_loss: 3.4281 - yolo_layer_2_loss: 5.5416 - val_loss: 76.5397 - val_yolo_layer_loss: 55.2744 - val_yolo_layer_1_loss: 8.6841 - val_yolo_layer_2_loss: 7.7411\n",
            "Epoch 17/30\n",
            "336/336 [==============================] - 280s 830ms/step - loss: 17.0461 - yolo_layer_loss: 3.6309 - yolo_layer_1_loss: 3.2824 - yolo_layer_2_loss: 5.3120 - val_loss: 16.2705 - val_yolo_layer_loss: 2.8771 - val_yolo_layer_1_loss: 3.7378 - val_yolo_layer_2_loss: 4.8831\n",
            "Epoch 18/30\n",
            "336/336 [==============================] - 292s 865ms/step - loss: 17.9861 - yolo_layer_loss: 4.6179 - yolo_layer_1_loss: 3.6813 - yolo_layer_2_loss: 4.9295 - val_loss: 15.9010 - val_yolo_layer_loss: 3.3665 - val_yolo_layer_1_loss: 2.8588 - val_yolo_layer_2_loss: 4.9721\n",
            "Epoch 19/30\n",
            "336/336 [==============================] - 272s 806ms/step - loss: 16.9796 - yolo_layer_loss: 3.7552 - yolo_layer_1_loss: 3.3973 - yolo_layer_2_loss: 5.1399 - val_loss: 17.2106 - val_yolo_layer_loss: 2.8783 - val_yolo_layer_1_loss: 4.4707 - val_yolo_layer_2_loss: 5.2258\n",
            "Epoch 20/30\n",
            "336/336 [==============================] - 300s 889ms/step - loss: 17.1889 - yolo_layer_loss: 4.4069 - yolo_layer_1_loss: 3.2626 - yolo_layer_2_loss: 4.9006 - val_loss: 18.8253 - val_yolo_layer_loss: 5.1602 - val_yolo_layer_1_loss: 4.0609 - val_yolo_layer_2_loss: 5.0296\n",
            "Epoch 21/30\n",
            "336/336 [==============================] - 274s 810ms/step - loss: 16.7062 - yolo_layer_loss: 4.0385 - yolo_layer_1_loss: 3.0177 - yolo_layer_2_loss: 5.0926 - val_loss: 17.6682 - val_yolo_layer_loss: 3.4764 - val_yolo_layer_1_loss: 3.3235 - val_yolo_layer_2_loss: 6.3572\n",
            "Epoch 22/30\n",
            "336/336 [==============================] - 280s 831ms/step - loss: 16.1947 - yolo_layer_loss: 3.9461 - yolo_layer_1_loss: 2.7348 - yolo_layer_2_loss: 5.0056 - val_loss: 15.1670 - val_yolo_layer_loss: 3.1959 - val_yolo_layer_1_loss: 3.0797 - val_yolo_layer_2_loss: 4.3926\n",
            "Epoch 23/30\n",
            "336/336 [==============================] - 270s 800ms/step - loss: 15.1478 - yolo_layer_loss: 3.3767 - yolo_layer_1_loss: 2.7644 - yolo_layer_2_loss: 4.5107 - val_loss: 14.3845 - val_yolo_layer_loss: 2.7099 - val_yolo_layer_1_loss: 2.9002 - val_yolo_layer_2_loss: 4.2869\n",
            "Epoch 24/30\n",
            "336/336 [==============================] - 269s 797ms/step - loss: 14.4342 - yolo_layer_loss: 3.1136 - yolo_layer_1_loss: 2.5068 - yolo_layer_2_loss: 4.3290 - val_loss: 14.2659 - val_yolo_layer_loss: 2.7073 - val_yolo_layer_1_loss: 2.9408 - val_yolo_layer_2_loss: 4.1408\n",
            "Epoch 25/30\n",
            "336/336 [==============================] - 272s 806ms/step - loss: 14.7599 - yolo_layer_loss: 3.5995 - yolo_layer_1_loss: 2.1637 - yolo_layer_2_loss: 4.5222 - val_loss: 14.7346 - val_yolo_layer_loss: 3.3441 - val_yolo_layer_1_loss: 2.9215 - val_yolo_layer_2_loss: 4.0016\n",
            "Epoch 26/30\n",
            "336/336 [==============================] - 288s 853ms/step - loss: 14.2771 - yolo_layer_loss: 3.4708 - yolo_layer_1_loss: 2.3696 - yolo_layer_2_loss: 3.9714 - val_loss: 14.6853 - val_yolo_layer_loss: 2.9789 - val_yolo_layer_1_loss: 3.0767 - val_yolo_layer_2_loss: 4.1710\n",
            "Epoch 27/30\n",
            "336/336 [==============================] - 271s 802ms/step - loss: 14.5427 - yolo_layer_loss: 3.6591 - yolo_layer_1_loss: 2.3152 - yolo_layer_2_loss: 4.1117 - val_loss: 14.9677 - val_yolo_layer_loss: 3.2635 - val_yolo_layer_1_loss: 3.2014 - val_yolo_layer_2_loss: 4.0521\n",
            "Epoch 28/30\n",
            "336/336 [==============================] - 272s 806ms/step - loss: 14.4567 - yolo_layer_loss: 3.7946 - yolo_layer_1_loss: 2.1068 - yolo_layer_2_loss: 4.1065 - val_loss: 14.0459 - val_yolo_layer_loss: 2.4784 - val_yolo_layer_1_loss: 2.5773 - val_yolo_layer_2_loss: 4.5474\n",
            "Epoch 29/30\n",
            "336/336 [==============================] - 276s 817ms/step - loss: 14.2712 - yolo_layer_loss: 3.6305 - yolo_layer_1_loss: 2.3496 - yolo_layer_2_loss: 3.8502 - val_loss: 14.3292 - val_yolo_layer_loss: 2.9283 - val_yolo_layer_1_loss: 2.9197 - val_yolo_layer_2_loss: 4.0464\n",
            "Epoch 30/30\n",
            "336/336 [==============================] - 276s 818ms/step - loss: 13.8658 - yolo_layer_loss: 3.2339 - yolo_layer_1_loss: 2.2349 - yolo_layer_2_loss: 3.9641 - val_loss: 15.3305 - val_yolo_layer_loss: 3.8242 - val_yolo_layer_1_loss: 3.2556 - val_yolo_layer_2_loss: 3.8235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate\n",
        "\n",
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/content/drive/MyDrive/imageai/LPDS\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/MyDrive/imageai/LPDS/models\", json_path=\"/content/drive/MyDrive/imageai/LPDS/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "IKrnSm0_jTCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Detector\n",
        "\n",
        "detector = CustomObjectDetection()\n",
        "detector.setModelTypeAsYOLOv3()\n",
        "detector.setModelPath(\"/content/drive/MyDrive/imageai/LPDS/models/detection_model-ex-029--loss-0014.191.h5\")\n",
        "detector.setJsonPath(\"/content/drive/MyDrive/imageai/LPDS/json/detection_config.json\")\n",
        "detector.loadModel()\n",
        "detections = detector.detectObjectsFromImage(input_image=\"/content/drive/MyDrive/imageai/LPDS/train/images/Cars100.png\", output_image_path=\"/content/drive/MyDrive/imageai/LP-detected.jpg\",  extract_detected_objects=True)\n",
        "#for detection in detections:\n",
        " #   print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])"
      ],
      "metadata": {
        "id": "oyfEsUp5Xunc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7065830e-a07a-4f88-ec09-bdddc4c3d93c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
            "/usr/local/lib/python3.7/dist-packages/imageai/Detection/Custom/__init__.py:1248: RuntimeWarning: overflow encountered in exp\n",
            "  return 1. / (1. + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = ImageClassification()\n",
        "\n",
        "base_model = tf.keras.applications.inception_v3.InceptionV3(\n",
        "    input_shape = (299, 299, 3),\n",
        "    include_top = True, \n",
        "    weights = \"imagenet\"                  \n",
        ")\n",
        "\n",
        "saving_path = pathlib.Path('/content/drive/MyDrive/imageai/SavedBaseModel.h5')\n",
        "\n",
        "base_model.save(saving_path)\n",
        "\n",
        "model_path = pathlib.Path('/content/drive/MyDrive/imageai/SavedBaseModel.h5')\n",
        "\n",
        "prediction.setModelTypeAsInceptionV3()\n",
        "\n",
        "prediction.setModelPath(\"/content/drive/MyDrive/imageai/SavedBaseModel.h5\")\n",
        "\n",
        "prediction.loadModel()\n",
        "\n",
        "predictions, probabilities = prediction.classifyImage(\"/content/drive/MyDrive/License_Plate_Recognition/LicPlateImages/10.png\", result_count=10)\n",
        "\n",
        "for eachPrediction, eachProbability in zip(predictions, probabilities):\n",
        "    print(eachPrediction , \" : \" , eachProbability)\n"
      ],
      "metadata": {
        "id": "OP4sm6UXmbe9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "LPR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}